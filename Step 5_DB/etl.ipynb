{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "from pymongo import MongoClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_html():\n",
    "    url = \"http://localhost:8000\"\n",
    "\n",
    "    r = requests.get(url)\n",
    "    html = r.text\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def extract_from_website(columns_headers):\n",
    "\n",
    "    soup = intialize_html()\n",
    "\n",
    "    tables = soup.find_all(\"tbody\")\n",
    "    rows = tables[0].find_all(\"tr\")\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for row in rows:\n",
    "        col = row.find_all(\"td\")\n",
    "\n",
    "        data_dict = {}\n",
    "        for i in range(len(columns_headers)):\n",
    "            data_dict[columns_headers[i]] = col[i].contents[0]\n",
    "            \n",
    "        # data_dict = {columns_headers[0]: col[0].contents[0],\n",
    "        #             columns_headers[1]: col[1].contents[0],\n",
    "        #             columns_headers[2]: col[2].contents[0],\n",
    "        #             columns_headers[3]: col[3].contents[0],\n",
    "        #             columns_headers[4]: col[4].contents[0],\n",
    "        #             columns_headers[5]: col[5].contents[0],\n",
    "        #             columns_headers[6]: col[6].contents[0],\n",
    "        #             columns_headers[7]: col[7].contents[0]\n",
    "        #             }\n",
    "        \n",
    "        data_list.append(data_dict)\n",
    "\n",
    "    df = pd.DataFrame(data_list, columns=columns_headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(csv_file):\n",
    "    dataframe = pd.read_csv(csv_file)\n",
    "    return dataframe\n",
    "\n",
    "def extract_from_json(json_file):\n",
    "    try:\n",
    "        return pd.read_json(json_file)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading {json_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(columns_headers):\n",
    "    extracted_data = pd.DataFrame(columns=columns_headers)\n",
    "    \n",
    "    website_data = extract_from_website(columns_headers)\n",
    "    extracted_data = pd.concat([extracted_data, website_data], ignore_index=True)\n",
    "\n",
    "    for csv_file in glob.glob(\"etl_files\\*.csv\"):\n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_csv(csv_file))], ignore_index=True)\n",
    "    \n",
    "    for json_file in glob.glob(\"etl_files\\*.json\"):\n",
    "        extracted_data = pd.concat([extracted_data, pd.DataFrame(extract_from_json(json_file))], ignore_index=True)\n",
    "\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "def transform(extracted_data):\n",
    "\n",
    "    numeric_columns = [\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\",\"Cost\"]\n",
    "    extracted_data[numeric_columns] = extracted_data[numeric_columns].astype(float)\n",
    "    extracted_data[[\"Id\"]] = extracted_data[[\"Id\"]].astype(int)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "def load_to_csv(df, csv_path):\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "def load_to_db_mySQL(df, my_sql_conn, table_name):\n",
    "    df.to_sql(table_name, my_sql_conn, if_exists='replace', index=False)\n",
    "\n",
    "def load_to_db_postgreSQL(df, postgres_conn, table_name):\n",
    "    df.to_sql(table_name, postgres_conn, if_exists='replace', index=False)\n",
    "\n",
    "def load_to_db_mongodb(df,collection):\n",
    "    data = df.to_dict(orient=\"records\")\n",
    "\n",
    "    collection.insert_many(data)\n",
    "\n",
    "def log_progress(message,log_file):\n",
    "    timestamp_format = \"%Y-%h-%d-%H:%M-%S\"\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(log_file,\"a\") as f:\n",
    "        f.write(timestamp + \",\" + message + \"\\n\")\n",
    "    \n",
    "\n",
    "def run_query(query_statement, conn):\n",
    "    # Running the query\n",
    "    with conn.connect() as connection:\n",
    "        result = connection.execute(text(query_statement))\n",
    "        for row in result:\n",
    "            print(row)\n",
    "\n",
    "def run_noSQL_query(query_statement, collection):\n",
    "    \n",
    "    # Query to find all documents\n",
    "    results = collection.find(query)\n",
    "\n",
    "    for doc in results:\n",
    "        print(doc)\n",
    "\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "log_file = \"log_file.txt\"\n",
    "columns_headers = [\"Id\",\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\",\"Species\",\"Diseases\",\"Cost\"]\n",
    "csv_path = \"dataset/dataset_final.csv\"\n",
    "\n",
    "#Never keep table_name as all CAPS\n",
    "table_name = \"iris\"\n",
    "MYSQL_CONNECTION_STRING = os.getenv('MYSQL_CONNECTION_STRING')\n",
    "POSTGRESQL_CONNECTION_STRING = os.getenv(\"POSTGRESQL_CONNECTION_STRING\")\n",
    "MONGODB_CONNECTION_STRING = os.getenv(\"MONGODB_CONNECTION_STRING\")\n",
    "\n",
    "MONGODB_DATABASE = os.getenv(\"MONGODB_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already Extracted\n"
     ]
    }
   ],
   "source": [
    "log_progress(\"Preliminaries complete. Initiating ETL process.\",log_file)\n",
    "if os.path.exists(csv_path) and os.path.getsize(csv_path) > 0:\n",
    "    print(\"Already Extracted\")\n",
    "    raw_extracted_data = pd.read_csv(csv_path)\n",
    "else:\n",
    "    raw_extracted_data = extract(columns_headers)\n",
    "\n",
    "log_progress(\"Preliminaries complete. Initiating ETL process.\",log_file)\n",
    "\n",
    "transformed_data = transform(raw_extracted_data)\n",
    "log_progress(\"Data transformation complete. Initiating loading process.\",log_file)\n",
    "\n",
    "load_to_csv(transformed_data,csv_path)\n",
    "log_progress(\"Data saved to CSV file.\",log_file)\n",
    "\n",
    "log_progress(\"SQL Connection initiated.\",log_file)\n",
    "my_sql_engine = create_engine(MYSQL_CONNECTION_STRING)\n",
    "\n",
    "load_to_db_mySQL(transformed_data,my_sql_engine,table_name)\n",
    "log_progress(\"Data loaded to MySQL Database as table.\",log_file)\n",
    "\n",
    "log_progress(\"PostGreSQL Connection initiated.\",log_file)\n",
    "postgreSQL_engine = create_engine(POSTGRESQL_CONNECTION_STRING)\n",
    "\n",
    "load_to_db_postgreSQL(transformed_data,postgreSQL_engine,table_name)\n",
    "log_progress(\"Data loaded to PostGreSQL Database as table.\",log_file)\n",
    "\n",
    "log_progress(\"MongoDB Connection initiated.\",log_file)\n",
    "client = MongoClient(MONGODB_CONNECTION_STRING)\n",
    "db = client[MONGODB_DATABASE]\n",
    "collection = db[table_name]\n",
    "\n",
    "load_to_db_mongodb(transformed_data,collection)\n",
    "log_progress(\"Data loaded to Mongodb Database as table.\",log_file)\n",
    "client.close()\n",
    "\n",
    "log_progress(\"Process Complete.\",log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"SELECT * FROM {table_name};\"\n",
    "run_query(query_statement=query,conn=postgreSQL_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(MONGODB_CONNECTION_STRING)\n",
    "db = client[MONGODB_DATABASE]\n",
    "collection = db[table_name]\n",
    "\n",
    "query = {\"Cost\": {\"$gt\": 150}}\n",
    "\n",
    "run_noSQL_query(query, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
